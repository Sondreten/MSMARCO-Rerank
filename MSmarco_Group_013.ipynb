{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team 13 MS MARCO Document re-ranking task\n",
    "\n",
    "Here you will find our implementation for the MS MARCO re-ranking 2020 task. This project was part of our evaluation and project work in DAT640-1 20H Information Retrieval and Text Mining. This implementation are using a feature based traditional machine learning approach to this problem.\n",
    "\n",
    "## Task\n",
    "\n",
    "We were to re-rank the top 100 documents retrieved from queries (from Indri model) in the MS MARCO dataset.\n",
    "We have implemented a both a baseline and an advanced model to tackle this problem.\n",
    "Our models are feature based traditional machine learning models with a pointwise learning to rank approach.\n",
    "\n",
    "## Environment\n",
    "\n",
    "Your environment is expected to be an Anaconda base environment with Python version 3.6+.\n",
    "You must have elasticsearch library installed with version 7.9.1+ and have a local instance of it running on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from elasticsearch import helpers\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "import concurrent.futures\n",
    "import more_itertools as mit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import numpy as np\n",
    "#from LambdaRankNN import LambdaRankNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To deal with csv error. https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DESKTOP-E2UM8IU',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '0MFc_C9cRA27Grr9lSUCYA',\n",
       " 'version': {'number': '7.9.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '083627f112ba94dffc1232e8b42b73492789ef91',\n",
       "  'build_date': '2020-09-01T21:22:21.964974Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set index to True to index the dataset. Requiers msmarco-docs.tsv and elasticsearch.\n",
    "index = False\n",
    "\n",
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME ='ms_marco_index'\n",
    "\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    \"settings\":{\n",
    "        \"analysis\":{\n",
    "       \n",
    "        \"analyzer\": {\n",
    "            \"my_analyzer\":{\n",
    "                \"tokenizer\":\"standard\",\n",
    "                \"filter\": [ \"lowercase\",\"kstem\",\"stop\" ]     \n",
    "            }         \n",
    "        }       \n",
    "            \n",
    "        }    \n",
    "        \n",
    "    },\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'url': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'title': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'my_analyzer'\n",
    "                },\n",
    "                'body': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'my_analyzer'\n",
    "                }\n",
    "                \n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "if index:\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function for bulk indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(file=\"msmarco-docs.tsv\"):\n",
    "    \"\"\"Create generator object containg document dictionaries for bulk indexing\n",
    "    Arguments:\n",
    "        file(string): name of file to index. \n",
    "    Returns:\n",
    "        Generator object \n",
    "    \"\"\"\n",
    "    with open(file,encoding='utf8') as f:\n",
    "        tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for [doc, url, title,body] in tsv:\n",
    "            yield {'_id':doc,'url':url,'title':title,'body':body}\n",
    "\n",
    "if index:\n",
    "    helpers.bulk(es, get_documents(), index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingdata\n",
    "\n",
    "In this section we will retrieve our downloaded datasets into our code and putting them into pandas dataframes for further use. Here we will also modify the dataframes to only get the columns we need to use for our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1185869</th>\n",
       "      <td>)what was the immediate impact of the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185868</th>\n",
       "      <td>_________ justice is designed to repair the ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183785</th>\n",
       "      <td>elegxo meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645590</th>\n",
       "      <td>what does physical medicine do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186154</th>\n",
       "      <td>feeding rice cereal how many times per day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     query\n",
       "id                                                        \n",
       "1185869  )what was the immediate impact of the success ...\n",
       "1185868  _________ justice is designed to repair the ha...\n",
       "1183785                                     elegxo meaning\n",
       "645590                      what does physical medicine do\n",
       "186154          feeding rice cereal how many times per day"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_querys_train = pd.read_csv(\"queries.doctrain.tsv\",encoding=\"utf8\",delimiter=\"\\t\",header=None)\n",
    "df_querys_train.rename( columns ={0: 'id', 1: 'query'}, inplace = True )\n",
    "df_querys_train = df_querys_train.astype('str')\n",
    "df_querys_train.set_index('id',inplace = True)\n",
    "df_querys_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>D312959</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>D140227</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>D213890</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>D1033338</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>D508131</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(3, D312959), (5, D140227), (12, D213890), (15, D1033338), (16, D508131)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qrels_train = pd.read_csv(\"msmarco-doctrain-qrels.tsv\",encoding=\"utf8\",delimiter=\"\\t\",header=None) \n",
    "df_qrels_train.rename( columns ={0: 'test'}, inplace = True )\n",
    "df_qrels_train = pd.DataFrame( df_qrels_train.test.str.split(' ',3).tolist(),columns = ['query_id','i1','doc_id','i2'])\n",
    "df_qrels_train.set_index(['query_id','doc_id'],inplace = True)\n",
    "df_qrels_train.drop(['i1', 'i2'], axis=1,inplace = True)\n",
    "df_qrels_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>i1</th>\n",
       "      <th>i2</th>\n",
       "      <th>score</th>\n",
       "      <th>i3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1185869</th>\n",
       "      <th>D59221</th>\n",
       "      <td>Q0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.80433</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D59220</th>\n",
       "      <td>Q0</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.92127</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2192591</th>\n",
       "      <td>Q0</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.05215</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2777518</th>\n",
       "      <td>Q0</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.05486</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2371978</th>\n",
       "      <td>Q0</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.07048</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   i1 i2     score                    i3\n",
       "query_id doc_id                                         \n",
       "1185869  D59221    Q0  1  -4.80433  IndriQueryLikelihood\n",
       "         D59220    Q0  2  -4.92127  IndriQueryLikelihood\n",
       "         D2192591  Q0  3  -5.05215  IndriQueryLikelihood\n",
       "         D2777518  Q0  4  -5.05486  IndriQueryLikelihood\n",
       "         D2371978  Q0  5  -5.07048  IndriQueryLikelihood"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top100_train = pd.read_csv(\"msmarco-doctrain-top100.tsv\",encoding=\"utf8\",delimiter=\"\\t\",header=None)#,nrows=1000)\n",
    "df_top100_train.rename( columns ={0: 'test'}, inplace = True )\n",
    "df_top100_train = pd.DataFrame( df_top100_train.test.str.split(' ',5).tolist(),columns = ['query_id','i1','doc_id','i2','score','i3'])\n",
    "df_top100_train.set_index(['query_id','doc_id'],inplace = True)\n",
    "df_top100_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev and Testdata\n",
    "\n",
    "In this section we will retrieve our downloaded development and training datasets into our code and putting them into pandas dataframes for further use. Here we will also modify the dataframes to only get the columns we need to use for our implementation like we did in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174249</th>\n",
       "      <td>does xpress bet charge to deposit money in you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320792</th>\n",
       "      <td>how much is a cost to run disneyland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090270</th>\n",
       "      <td>botulinum definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101279</th>\n",
       "      <td>do physicians pay for insurance from their sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201376</th>\n",
       "      <td>here there be dragons comic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     query\n",
       "id                                                        \n",
       "174249   does xpress bet charge to deposit money in you...\n",
       "320792                how much is a cost to run disneyland\n",
       "1090270                               botulinum definition\n",
       "1101279  do physicians pay for insurance from their sal...\n",
       "201376                         here there be dragons comic"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_querys_devtest = pd.read_csv(\"queries.docdev.tsv\",encoding=\"utf8\",delimiter=\"\\t\",header=None)\n",
    "df_querys_devtest.rename( columns ={0: 'id', 1: 'query'}, inplace = True )\n",
    "df_querys_devtest = df_querys_devtest.astype('str')\n",
    "df_querys_devtest.set_index('id',inplace = True)\n",
    "df_querys_devtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>i1</th>\n",
       "      <th>i2</th>\n",
       "      <th>score</th>\n",
       "      <th>i3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">174249</th>\n",
       "      <th>D3126539</th>\n",
       "      <td>Q0</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.99003</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D978773</th>\n",
       "      <td>Q0</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.18444</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D399803</th>\n",
       "      <td>Q0</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.20982</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2204704</th>\n",
       "      <td>Q0</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.24312</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3126541</th>\n",
       "      <td>Q0</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.24726</td>\n",
       "      <td>IndriQueryLikelihood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   i1 i2     score                    i3\n",
       "query_id doc_id                                         \n",
       "174249   D3126539  Q0  1  -5.99003  IndriQueryLikelihood\n",
       "         D978773   Q0  2  -6.18444  IndriQueryLikelihood\n",
       "         D399803   Q0  3  -6.20982  IndriQueryLikelihood\n",
       "         D2204704  Q0  4  -6.24312  IndriQueryLikelihood\n",
       "         D3126541  Q0  5  -6.24726  IndriQueryLikelihood"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top100_devtest = pd.read_csv(\"docdev-stopstem.xml_1.out\",encoding=\"utf8\",delimiter=\"\\t\",header=None)#,nrows=1000)\n",
    "df_top100_devtest.rename( columns ={0: 'test'}, inplace = True )\n",
    "df_top100_devtest = pd.DataFrame( df_top100_devtest.test.str.split(' ',5).tolist(),columns = ['query_id','i1','doc_id','i2','score','i3'])\n",
    "df_top100_devtest.set_index(['query_id','doc_id'],inplace = True)\n",
    "#df_qrels.drop(['i1', 'i2'], axis=1,inplace = True)\n",
    "df_top100_devtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>D1650436</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <th>D1202771</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <th>D1547717</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <th>D1313702</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <th>D2113408</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(2, D1650436), (1215, D1202771), (1288, D1547717), (1576, D1313702), (2235, D2113408)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qrels_devtest = pd.read_csv(\"msmarco-docdev-qrels.tsv\",encoding=\"utf8\",delimiter=\"\\t\",header=None) \n",
    "df_qrels_devtest.rename( columns ={0: 'test'}, inplace = True )\n",
    "df_qrels_devtest = pd.DataFrame( df_qrels_devtest.test.str.split(' ',3).tolist(),columns = ['query_id','i1','doc_id','i2'])\n",
    "df_qrels_devtest.set_index(['query_id','doc_id'],inplace = True)\n",
    "df_qrels_devtest.drop(['i1', 'i2'], axis=1,inplace = True)\n",
    "df_qrels_devtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev:  4693\n",
      "Test:  500\n"
     ]
    }
   ],
   "source": [
    "df_querys_dev = df_querys_devtest[500:]#.sample(frac=.95,random_state=2)\n",
    "df_querys_test = df_querys_devtest.drop(df_querys_dev.index)\n",
    "df_top100_dev = df_top100_devtest.loc[df_querys_dev.index] \n",
    "df_top100_test =df_top100_devtest.loc[df_querys_test.index]\n",
    "df_qrels_dev = df_qrels_devtest.loc[df_querys_dev.index]\n",
    "df_qrels_test = df_qrels_devtest.loc[df_querys_test.index]\n",
    "\n",
    "print('Dev: ',len(df_querys_dev))\n",
    "print('Test: ',len(df_querys_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate \n",
    "\n",
    "In this section we will implement the function to rank the MRR@100 score for our implementations. The function is gathered from the official MS MARCO website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Official scoring function from: https://github.com/microsoft/MSMARCO-Document-Rankin,ms_marco_eval.py \n",
    "def compute_metrics(qids_to_relevant_documentids, qids_to_ranked_candidate_documents):\n",
    "    \"\"\"Compute MRR metric\n",
    "    Args:    \n",
    "    p_qids_to_relevant_documentids (dict): dictionary of query-document mapping\n",
    "        Dict as read in with load_reference or load_reference_from_stream\n",
    "    p_qids_to_ranked_candidate_documents (dict): dictionary of query-document candidates\n",
    "    Returns:\n",
    "        dict: dictionary of metrics {'MRR': <MRR Score>}\n",
    "    \"\"\"\n",
    "    all_scores = {}\n",
    "    MRR = 0\n",
    "    qids_with_relevant_documents = 0\n",
    "    ranking = []\n",
    "    \n",
    "    for qid in qids_to_ranked_candidate_documents:\n",
    "        if qid in qids_to_relevant_documentids:\n",
    "            ranking.append(0)\n",
    "            target_pid = qids_to_relevant_documentids[qid]\n",
    "            candidate_pid = qids_to_ranked_candidate_documents[qid]\n",
    "            for i in range(0,len(candidate_pid)):\n",
    "                if candidate_pid[i] in target_pid:\n",
    "                    MRR += 1/(i + 1)\n",
    "                    ranking.pop()\n",
    "                    ranking.append(i+1)\n",
    "                    break\n",
    "    if len(ranking) == 0:\n",
    "        raise IOError(\"No matching QIDs found. Are you sure you are scoring the evaluation set?\")\n",
    "    \n",
    "    MRR = MRR/len(qids_to_relevant_documentids)\n",
    "    all_scores['MRR @100'] = MRR\n",
    "    all_scores['QueriesRanked'] = len(set(qids_to_ranked_candidate_documents))\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indri: {'MRR @100': 0.22037977500707348, 'QueriesRanked': 500}\n"
     ]
    }
   ],
   "source": [
    "truth_test = {}\n",
    "rank_indri ={}\n",
    "for query in df_top100_test.index.get_level_values(0).unique():\n",
    "    truth_test[query] = df_qrels_test.loc[query].index.tolist()\n",
    "    rank_indri[query] = df_top100_test.loc[query].index.tolist()\n",
    "\n",
    "result = compute_metrics(truth_test,rank_indri)\n",
    "print('Indri:',result  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "## Functions from Assignment A5 DAT640 University of Savanger fall 2020:\n",
    "\n",
    "Here we will implement the functions used to retrieve the features for our baseline model and functions for ranking. Many of these functions are taken from Assignment: A5 in the course DAT640-1 20H Information Retrieval and Text Mining by Krisztian Balog. The functions taken from this assignment is marked with '#A5 DAT640 UiS 2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A5 DAT640 UiS 2020\n",
    "def analyze_query(es, query, field, index=INDEX_NAME):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query,\"tokenizer\": \"standard\",\"filter\": [ \"lowercase\",\"kstem\",\"stop\" ]})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "#A5 DAT640 UiS 2020\n",
    "def extract_doc_features(doc_id, es, index=INDEX_NAME):\n",
    "    \"\"\"Extracts features of a document.\n",
    "    \n",
    "        Arguments:\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'doc_length_title', 'doc_length_body'.\n",
    "    \"\"\"\n",
    "    doc_features = {}\n",
    "    \n",
    "    vec = es.termvectors(index=index, id=doc_id,term_statistics=True)['term_vectors']\n",
    "    title = vec['title']['terms'] if 'title' in vec.keys() else 0\n",
    "    body = vec['body']['terms'] if 'body' in vec.keys() else 0\n",
    "    \n",
    "    doc_features['doc_length_title'] = sum([int(title[term]['term_freq']) for term in title ]) if title!=0 else 0\n",
    "    doc_features['doc_length_body'] = sum([int(body[term]['term_freq']) for term in body ]) if body!=0 else 0\n",
    "\n",
    "    return doc_features\n",
    "\n",
    "#A5 DAT640 UiS 2020\n",
    "def rerank(ltr,df_ranked,querys,es =es,index =INDEX_NAME,  fields = ['title','body'],df =df_querys_test):\n",
    "\n",
    "    \n",
    "    test_rankings={}\n",
    "    for i,query in enumerate(querys):\n",
    "            \n",
    "            X=[]\n",
    "            for doc in df_ranked.loc[query].index.tolist():\n",
    "                f = get_features_baseline(df_querys_test.loc[query]['query'], doc, es, index,fields)\n",
    "                X.append(f)\n",
    "\n",
    "            \n",
    "            r = ltr.rank(X, df_ranked.loc[query].index.tolist())\n",
    "            \n",
    "            test_rankings[query] = [i[0] for i in r]\n",
    "            \n",
    "            if i%2==0:\n",
    "                    print(round((i/len(querys))*100,3),'%',end='\\r')\n",
    "    \n",
    "    print(round((i/len(querys))*100,3),'%',end='\\r')               \n",
    "    return test_rankings\n",
    "\n",
    "#A5 DAT640 UiS 2020\n",
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classifier: An instance of scikit-learn regressor.\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"Trains an LTR model.\n",
    "        \n",
    "        Arguments:\n",
    "            X: Features of training instances.\n",
    "            y: Relevance assessments of training instances.\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"Predicts relevance labels and rank documents for a given query.\n",
    "        \n",
    "        Arguments:\n",
    "            ft: A list of feature vectors for query-document pairs.\n",
    "            doc_ids: A list of document ids.\n",
    "        Returns:\n",
    "            List of tuples, each consisting of document ID and predicted relevance label.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        \n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unordered_bigram_matches(query,text):\n",
    "        \"\"\"Count number of sentences with more then two query matches\n",
    "        Args:    \n",
    "        query (list):  list of query tokens\n",
    "        text (string) text to \n",
    "        Returns:\n",
    "            dict: dictionary of metrics {'hits':number ,'length':number of sentences }\n",
    "        \"\"\"\n",
    "        t_sentence = list(sent_tokenize(text))\n",
    "        sentences = [get_tokenized(sentence) for sentence in t_sentence]\n",
    "        sent = sum([1 for sentence in sentences if len(set(sentence).intersection(query))>=2 ])\n",
    "        return {'hits':sent,'length':len(t_sentence)} \n",
    "    \n",
    "\n",
    "def get_tokenized(text,limit=40000):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        text (string): text to be tokenized  \n",
    "        limit(int): cap the text length used\n",
    "        Returns:\n",
    "            list contaning tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = es.indices.analyze(index=INDEX_NAME, body={'text': text[:limit],\"tokenizer\": \"standard\",\"filter\": [ \"lowercase\",\"kstem\",\"stop\" ]})['tokens']\n",
    "        return [ token['token'] for token in  sorted(tokens, key=lambda x: x['position']) ]\n",
    "\n",
    "def get_ngram_matches(query,text,n_grams):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        query (list): query tokens  \n",
    "        text(string): text\n",
    "        n_gram: number of n gram words to use.\n",
    "        Returns:\n",
    "             dict: dictionary of metrics{'unique_query_terms':int,'sum_TF':int,'max_TF':int,'avg_TF':int,'len':int }\n",
    "        \"\"\"\n",
    "    \n",
    "        features ={'unique_query_terms':0,'sum_TF':0,'max_TF':0,'avg_TF':0,'len':0 }\n",
    "        \n",
    "        if len(query)<n_grams:\n",
    "            return features\n",
    "        \n",
    "        q_ngrams = list(ngrams(query,n_grams))\n",
    "        t_ngrams = list(ngrams(text,n_grams))\n",
    "        \n",
    "        terms = [ t_ngrams.count(query_term) if query_term in t_ngrams else 0 for query_term in q_ngrams ]\n",
    "        features['unique_query_terms'] = len( set([t  for t in  q_ngrams if t in t_ngrams]) )\n",
    "        features['sum_TF'] =sum(terms)\n",
    "        features['max_TF'] = max(terms) if sum(terms)!=0 else 0\n",
    "        features['avg_TF'] = sum(terms)/len(terms) if sum(terms)!=0 else 0\n",
    "        features['len'] = len(t_ngrams)\n",
    "        \n",
    "        return features    \n",
    "            \n",
    "def get_unigrams(query_terms, doc_id,field ,es, index=INDEX_NAME):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        query_terms (list): query tokens  \n",
    "        doc_id(string): id of document\n",
    "        field(string): name of field to find matches\n",
    "        es(): elasticsearch obj\n",
    "        index(string): name of index\n",
    "        Returns:\n",
    "             dict: dictionary of metrics{'unique_query_terms':int,'sum_TF':int,'max_TF':int,'avg_TF':int,'len':int }\n",
    "        \"\"\"\n",
    "\n",
    "        features = {'unique_query_terms':0,'sum_TF':0,'max_TF':0,'avg_TF':0,'len':0 }\n",
    "\n",
    "        tv = es.termvectors(index=index,id=doc_id,field_statistics=True,term_statistics=True)['term_vectors']\n",
    "\n",
    "        if field not in tv.keys():\n",
    "            return features\n",
    "\n",
    "        terms =  [tv[field]['terms'][t]['term_freq'] if t in tv[field]['terms'].keys() else 0  for t in  query_terms[field]  ]\n",
    "        features['unique_query_terms'] = len( set([t  for t in  query_terms[field] if t in tv[field]['terms'].keys() ]) )\n",
    "        features['sum_TF'] =sum(terms)\n",
    "        features['max_TF'] = max(terms) if sum(terms)!=0 else 0\n",
    "        features['avg_TF'] = sum(terms)/len(terms) if sum(terms)!=0 else 0\n",
    "        features['len'] = int(sum([int(tv[field]['terms'][term]['term_freq']) for term in tv[field]['terms'].keys() ]) if terms!=0 else 0)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors_baseline(query,query_id,top_100_docs,elastic=es, index=INDEX_NAME, fields = ['title','body']):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        query (string): query string  \n",
    "        query_id(string): query id\n",
    "        top_100_docs(list): list of document ids\n",
    "        elastic(): elastic obj\n",
    "        index(string):index name \n",
    "        Returns:\n",
    "             numpy array: feauter vectors\n",
    "             tuple: (query_id,doc_id) \n",
    "        \"\"\"\n",
    "    \n",
    "        X = []\n",
    "        ids = []\n",
    "        for doc in top_100_docs:\n",
    "            X.append( get_features_baseline(str(query),str(doc), elastic, index,))\n",
    "            ids.append((str(query_id),str(doc)))\n",
    "        \n",
    "        return np.array(X),ids \n",
    "    \n",
    "    \n",
    "\n",
    "def get_features_baseline(query, doc_id, es, index='ms_marco_index', fields =['title','body']):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        query (string): query string  \n",
    "        query_id(string): query id\n",
    "        elastic(): elastic obj\n",
    "        index(string):index name \n",
    "        Returns:\n",
    "             numpy array: feauter vector \n",
    "        \"\"\"\n",
    "        \n",
    "        #print(doc_id)\n",
    "        doc_orig = es.get(id= doc_id,index=INDEX_NAME)['_source']\n",
    "       \n",
    "    \n",
    "        #Query Document features\n",
    "        query_terms = {field: get_tokenized(analyze_query(es,str(query),field,index)) for field in fields}\n",
    "        terms =  { field: get_tokenized(doc_orig[field]) for field in fields }\n",
    "        unigram = {field:get_unigrams(query_terms, doc_id,field ,es, index ) for field in fields}\n",
    "        bigram = {field:get_ngram_matches(query_terms[field],terms[field],2) for field in fields}\n",
    "        trigram = {field:get_ngram_matches(query_terms[field],terms[field],3) for field in fields}  \n",
    "        unordered_bigram_body = get_unordered_bigram_matches(query_terms['body'],doc_orig['body']) \n",
    "        \n",
    "        \n",
    "        #Query features\n",
    "        q= np.array([ len(query_terms[field]) for field in fields] ).flatten()\n",
    "       \n",
    "    \n",
    "        #document feautures\n",
    "        d = extract_doc_features(doc_id,es)\n",
    "        \n",
    "        doc = np.array([d[val] for val in d]).flatten()\n",
    "        uni = np.array([ [ unigram[field][val_type] for val_type in unigram[field]] for field in unigram ]).flatten()\n",
    "        bi = np.array([ [ bigram[field][val_type] for val_type in bigram[field]] for field in bigram ]).flatten()\n",
    "        tri = np.array([ [ trigram[field][val_type] for val_type in trigram[field]] for field in trigram ]).flatten()\n",
    "        unor = np.array( [unordered_bigram_body[i] for i in  unordered_bigram_body ]  ).flatten()\n",
    "       \n",
    "        return np.concatenate(( q,doc,uni,bi,tri,unor), axis=None) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_trainingdata_baseline(df_querys,df_top100,df_qrels,number_of_neg_samples=1, es=es, index=INDEX_NAME):\n",
    "            \"\"\" \n",
    "            Args:    \n",
    "            df_querys (pandas dataframe): \n",
    "            df_top100 (pandas dataframe): \n",
    "            df_qrels (pandas dataframe):\n",
    "            number of neg samples(int): How many negative samples to include \n",
    "            es(): elastic obj\n",
    "            index(string):index name \n",
    "            Returns:\n",
    "                 numpy array: feauter vector X\n",
    "                 list : lables y\n",
    "                 tuple: (query_id,doc_id)\n",
    "            \"\"\"\n",
    "            \n",
    "            X = []\n",
    "            y = []\n",
    "            q = []\n",
    "            ids = [] \n",
    "            for i,query in enumerate(df_querys.index.tolist()):\n",
    "                \n",
    "                top_100   = df_top100.loc[query].index.tolist()\n",
    "                relevant  = df_qrels.loc[query].index.tolist()\n",
    "                \n",
    "                pos_all = [e for i,e in enumerate(top_100)  if e in relevant]    \n",
    "                neg_all = [e for i,e in enumerate(top_100)  if e not in relevant]\n",
    "                \n",
    "                if not pos_all:\n",
    "                    continue\n",
    "                    \n",
    "                query_text = df_querys.loc[query]['query']\n",
    "                \n",
    "                neg_samples = neg_all[-number_of_neg_samples:] #random.sample(neg_all, number_of_neg_samples)\n",
    "                features_neg,dq_ids  = get_feature_vectors_baseline(query_text,query,neg_samples)\n",
    "                \n",
    "                for vec,id_ in zip(features_neg,dq_ids):\n",
    "                    X.append(vec)\n",
    "                    y.append(0)\n",
    "                    ids.append(id_ )\n",
    "                    \n",
    "                    \n",
    "                features_pos,dq_ids  = get_feature_vectors_baseline(query_text,query,pos_all)\n",
    "                \n",
    "                for vec,id_ in zip(features_pos,dq_ids):\n",
    "                    X.append(vec)\n",
    "                    y.append(1)\n",
    "                    ids.append(id_)\n",
    "                \n",
    "                if i%10==0:\n",
    "                    print(round((i/len(training_querys))*100,3),'%',end='\\r')\n",
    "            \n",
    "            print(round((i/len(training_querys))*100,3),'%',end='\\r')        \n",
    "            return X,y,ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this section we will use the functions we have created earlier in this code to train our data using the RandomForestRegressor and PointWiseLTRModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99 %\r"
     ]
    }
   ],
   "source": [
    "training_querys = df_querys_train.sample(n=10000)#.index.get_level_values(0).tolist()\n",
    "X_train_baseline,y_train_baseline,ids_baseline = get_trainingdata_baseline(training_querys,df_top100_train,df_qrels_train,number_of_neg_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr._train(X_train_baseline, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'MRR @100': 0.22659855588013, 'QueriesRanked': 500}\n"
     ]
    }
   ],
   "source": [
    "reranked_baseline = rerank(ltr,df_top100_test,df_querys_test.index.tolist(),es =es,index =INDEX_NAME,  fields = ['title','body'],df =df_querys_test)\n",
    "result = compute_metrics(truth_test,reranked_baseline)\n",
    "print('Baseline:',result  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model\n",
    "\n",
    "In this section we will add functions to add additional features to our baseline model. Lastly we will rank this advanced model and evaluate it, to see how it scored compared to the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_match(url,query):\n",
    "        url = url.replace('com','').replace('www','').replace('html','').replace('en','').split('/')\n",
    "        test = ' '.join([ word.replace('.',' ').replace('_',' ') +' ' for word in url[1:]])\n",
    "        url_tokens = get_tokenized(test)\n",
    "        \n",
    "        return len(set(url_tokens).intersection(query))\n",
    "\n",
    "    \n",
    "def get_feature_vectors_advanced(query_id,df_querys,df_top100,elastic=es, index=INDEX_NAME, fields = ['title','body']):\n",
    "        \"\"\" \n",
    "        Args:    \n",
    "        query (string): query string  \n",
    "        query_id(string): query id\n",
    "        top_100_docs(list): list of document ids\n",
    "        elastic(): elastic obj\n",
    "        index(string):index name \n",
    "        Returns:\n",
    "             numpy array: feauter vectors\n",
    "             tuple: (query_id,doc_id) \n",
    "        \"\"\"\n",
    "    \n",
    "        X = []\n",
    "        ids = []\n",
    "        #print(df_top100.head())\n",
    "        for doc in df_top100.index.tolist():\n",
    "            X.append( get_features_advanced(doc,query_id,df_querys,df_top100, elastic, index))\n",
    "            ids.append((str(query_id),str(doc)))\n",
    " \n",
    "        return np.array(X),ids    \n",
    "\n",
    "def get_features_advanced(doc_id,query_id,df_querys,df_top100, es, index=INDEX_NAME, fields =['title','body']):\n",
    "        \n",
    "        #print(doc_id)\n",
    "        doc_orig = es.get(id= doc_id,index=INDEX_NAME)['_source']\n",
    "       \n",
    "        score = float(df_top100.loc[doc_id]['score'])\n",
    "        \n",
    "        #Query Document features\n",
    "        query_terms = {field: get_tokenized(analyze_query(es,str(df_querys.loc[query_id]['query']),field,index)) for field in fields}\n",
    "        terms =  { field: get_tokenized(doc_orig[field]) for field in fields }\n",
    "        unigram = {field:get_unigrams(query_terms, doc_id,field ,es, index ) for field in fields}\n",
    "        bigram = {field:get_ngram_matches(query_terms[field],terms[field],2) for field in fields}\n",
    "        trigram = {field:get_ngram_matches(query_terms[field],terms[field],3) for field in fields} \n",
    "        match = {field:get_ngram_matches(query_terms[field],terms[field],len(query_terms[field])) for field in fields}\n",
    "        unordered_bigram_body = get_unordered_bigram_matches(query_terms['body'],doc_orig['body']) \n",
    "        \n",
    "        \n",
    "        #Query features\n",
    "        q= np.array([ len(query_terms[field]) for field in fields] ).flatten()\n",
    "       \n",
    "    \n",
    "        #document feautures\n",
    "        d = extract_doc_features(doc_id,es)\n",
    "        \n",
    "        mat = np.array([ [ match[field][val_type] for val_type in match[field]] for field in match ]).flatten()\n",
    "        doc = np.array([d[val] for val in d]).flatten()\n",
    "        uni = np.array([ [ unigram[field][val_type] for val_type in unigram[field]] for field in unigram ]).flatten()\n",
    "        bi = np.array([ [ bigram[field][val_type] for val_type in bigram[field]] for field in bigram ]).flatten()\n",
    "        tri = np.array([ [ trigram[field][val_type] for val_type in trigram[field]] for field in trigram ]).flatten()\n",
    "        unor = np.array( [unordered_bigram_body[i] for i in  unordered_bigram_body ]  ).flatten()\n",
    "        url = url_match(doc_orig['url'],query_terms)\n",
    "        \n",
    "       \n",
    "        return np.concatenate(( score,q,doc,uni,bi,tri,url,unor,mat), axis=None) \n",
    "    \n",
    "    \n",
    "def get_trainingdata_advanced(df_querys,df_top100,df_qrels,number_of_neg_samples=1, elastic=es, index=INDEX_NAME):\n",
    "            \"\"\" \n",
    "            Args:    \n",
    "            df_querys (pandas dataframe): \n",
    "            df_top100 (pandas dataframe): \n",
    "            df_qrels (pandas dataframe):\n",
    "            number of neg samples(int): How many negative samples to include \n",
    "            es(): elastic obj\n",
    "            index(string):index name \n",
    "            Returns:\n",
    "                 numpy array: feauter vector X\n",
    "                 list : lables y\n",
    "                 tuple: (query_id,doc_id)\n",
    "            \"\"\"\n",
    "            \n",
    "            X = []\n",
    "            y = []\n",
    "            q = []\n",
    "            ids = [] \n",
    "            for i,query in enumerate(df_querys.index.tolist()):\n",
    "                #print(query)\n",
    "                top_100   = df_top100.loc[query].index.tolist()\n",
    "                relevant  = df_qrels.loc[query].index.tolist()\n",
    "                \n",
    "                pos_all = [e for i,e in enumerate(top_100)  if e in relevant]    \n",
    "                neg_all = [e for i,e in enumerate(top_100)  if e not in relevant]\n",
    "                \n",
    "                if not pos_all:\n",
    "                    continue\n",
    "                    \n",
    "                query_text = df_querys.loc[query]['query']\n",
    "                \n",
    "                neg_samples = neg_all[-number_of_neg_samples:] # query,df_querys,df_top100,elasitic,index\n",
    "                #print(df_top100.loc[query].loc[neg_samples])\n",
    "                \n",
    "                features_neg,dq_ids  = get_feature_vectors_advanced(query,df_querys,df_top100.loc[query].loc[neg_samples],elastic,index)\n",
    "                \n",
    "                for vec,id_ in zip(features_neg,dq_ids):\n",
    "                    X.append(vec)\n",
    "                    y.append(0)\n",
    "                    ids.append(id_ )\n",
    "                    \n",
    "                    \n",
    "                features_pos,dq_ids  = get_feature_vectors_advanced(query,df_querys,df_top100.loc[query].loc[pos_all],elastic,index)\n",
    "                \n",
    "                for vec,id_ in zip(features_pos,dq_ids):\n",
    "                    X.append(vec)\n",
    "                    y.append(1)\n",
    "                    ids.append(id_)\n",
    "                \n",
    "                if i%10==0:\n",
    "                    print(round((i/len(training_querys))*100,3),'%',end='\\r')\n",
    "            \n",
    "            print(round((i/len(training_querys))*100,3),'%',end='\\r')        \n",
    "            return X,y,ids  \n",
    "        \n",
    "        \n",
    "def rerank_advanced(ltr,df_ranked,querys,elasitic =es,index =INDEX_NAME,  fields = ['title','body'],df =df_querys_test):\n",
    "\n",
    "    \n",
    "    test_rankings={}\n",
    "    for i,query in enumerate(querys):\n",
    "            \n",
    "            X=[]\n",
    "            for doc in df_ranked.loc[query].index.tolist():\n",
    "                f = get_features_baseline(df_querys_test.loc[query]['query'], doc, elasitic, index,fields)\n",
    "                X.append(f)\n",
    "            \n",
    "            X, _ =get_feature_vectors_advanced(query,df,df_ranked.loc[query],elasitic,index)\n",
    "            \n",
    "            r = ltr.rank(X, df_ranked.loc[query].index.tolist())\n",
    "            \n",
    "            test_rankings[query] = [i[0] for i in r]\n",
    "            \n",
    "            if i%2==0:\n",
    "                    print(round((i/len(querys))*100,3),'%',end='\\r')\n",
    "    \n",
    "    print(round((i/len(querys))*100,3),'%',end='\\r')               \n",
    "    return test_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99 %\r"
     ]
    }
   ],
   "source": [
    "training_querys = df_querys_train.sample(n=10000)#.index.get_level_values(0).tolist()\n",
    "X_train,y_train,pair = get_trainingdata_advanced(training_querys,df_top100_train,df_qrels_train,number_of_neg_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr._train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced: {'MRR @100': 0.24852538544781397, 'QueriesRanked': 500}\n"
     ]
    }
   ],
   "source": [
    "reranked_advanced = rerank_advanced(ltr,df_top100_test,df_querys_test.index.tolist(),elasitic =es,index =INDEX_NAME,  fields = ['title','body'],df =df_querys_test)\n",
    "result = compute_metrics(truth_test,reranked_advanced)\n",
    "print('Advanced:',result  ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
